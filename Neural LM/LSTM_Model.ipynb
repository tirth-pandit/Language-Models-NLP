{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gXtYZuOgyqWU"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch\n",
    "import os\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "NkuPw3qF0qMZ"
   },
   "outputs": [],
   "source": [
    "para = {}\n",
    "para['seq'] = 5\n",
    "para['batch'] = 80\n",
    "para['lr'] = 0.005\n",
    "para['hidden'] = 1024\n",
    "para['layers'] = 1\n",
    "para['embed'] = 256\n",
    "para['epoch'] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sJD73Q3E0tRK"
   },
   "outputs": [],
   "source": [
    "def text_cleaner(stmts):\n",
    "    final = []\n",
    "    \n",
    "    for text in stmts:\n",
    "       \n",
    "        newString = text.lower()\n",
    "        newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "       \n",
    "        newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "        long_words=[]\n",
    "        \n",
    "        for i in newString.split():\n",
    "            if len(i)>=0:                  \n",
    "                long_words.append(i)\n",
    "        final.append( (\" \".join(long_words)).strip() ) \n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dA_J8CxQ0u6N"
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "\n",
    "    f =  open('brown.txt')\n",
    "    l = f.readlines()\n",
    "\n",
    "    final_lines = []\n",
    "    temp = ''\n",
    "\n",
    "    for i in l:\n",
    "\n",
    "        if len(i.split('\\n')[0]) != 0:\n",
    "            temp = ' '.join([temp , i.split('\\n')[0]])\n",
    "        else:\n",
    "            final_lines.append(temp)\n",
    "            temp = ''\n",
    "    \n",
    "    return final_lines\n",
    "\n",
    "sentences = get_data('brown.txt')\n",
    "sentences = text_cleaner(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5460\n"
     ]
    }
   ],
   "source": [
    "print( len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7pvWoPZG1PNB"
   },
   "outputs": [],
   "source": [
    "def get_dict(data):\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    idx = 0 \n",
    "    total = 0 \n",
    "\n",
    "    for line in data:\n",
    "        words = ['<s>'] + line.split() + ['</s>']\n",
    "\n",
    "        total += len(words)\n",
    "\n",
    "        for w in words:\n",
    "            if w not in final.keys() :\n",
    "                final[w] = idx \n",
    "                idx += 1\n",
    "    \n",
    "    final['<UNK>'] = idx\n",
    "    return final , total \n",
    "\n",
    "word_dict , total_tokens = get_dict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IuL74MwS09H2"
   },
   "outputs": [],
   "source": [
    "total_stmt = len(sentences)\n",
    "\n",
    "train_len = int( (7*total_stmt)/10)\n",
    "valid_len = int( (2*total_stmt)/10)\n",
    "test_len = int( (1*total_stmt)/10)\n",
    "\n",
    "train_data = sentences[:train_len]\n",
    "val_data = sentences[train_len:train_len+valid_len]\n",
    "test_data = sentences[train_len+valid_len : train_len+valid_len+test_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6m4ho34V1Xt1"
   },
   "outputs": [],
   "source": [
    "def get_int_data(data ,total_token):\n",
    "\n",
    "    ids = torch.LongTensor(total_token)\n",
    "    token = 0\n",
    "    \n",
    "    for line in data:\n",
    "        words = ['<s>'] + line.split() + ['</s>']\n",
    "\n",
    "        for w in words:\n",
    "            ids[token] = word_dict[w]\n",
    "            token += 1\n",
    "    \n",
    "    num_batches = ids.size(0) // para['batch']\n",
    "    ids = ids[:num_batches*para['batch'] ]\n",
    "    return ids.view(para['batch'], -1) \n",
    "\n",
    "train_int  = get_int_data(train_data, total_tokens)\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "num_batches = train_int.size(1) // para['seq']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "9_HbRzrj1ZnV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neural_LM(\n",
       "  (embed): Embedding(41505, 256)\n",
       "  (lstm): LSTM(256, 1024, batch_first=True)\n",
       "  (linear): Linear(in_features=1024, out_features=41505, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def resize_outputs( arr):\n",
    "    dim1 = arr.size(0) * arr.size(1)\n",
    "    dim2 = arr.size(2)\n",
    "\n",
    "    temp = arr.reshape( dim1 ,dim2)\n",
    "    return temp\n",
    "    \n",
    "def define_model(embed , hidden , layer , vocab_size ,opt , lrate):\n",
    "    device = 'cuda'\n",
    "    model = Neural_LM(embed, hidden, layer ,vocab_size).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if opt =='adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lrate)\n",
    "    elif opt == 'adelta':\n",
    "        optimizer = torch.optim.Adadelta(model.parameters() ,lr = lrate)\n",
    "    elif opt == 'agrad':\n",
    "        optimizer = torch.optim.Adagrad(model.parameters, lr=lrate)\n",
    "    \n",
    "    return model ,device ,criterion ,optimizer\n",
    "        \n",
    "class Neural_LM(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed , hid , layers ,vocab):\n",
    "        \n",
    "        super(Neural_LM, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab, embed)\n",
    "        self.lstm = nn.LSTM(embed, hid , layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hid, vocab )\n",
    "\n",
    "    def forward(self, inputs , hidden_var):\n",
    "        op = self.embed(inputs)\n",
    "\n",
    "        op, (hidden_var, temp) = self.lstm(op, hidden_var)\n",
    "        \n",
    "        op = resize_outputs(op)\n",
    "\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output, (hidden_var, temp)\n",
    "    \n",
    "    def get_init_stat(layer , batch ,hid )\n",
    "        \n",
    "        stat = (torch.zeros(layer ,batch ,hid).to(device),\n",
    "              torch.zeros( layer ,batch ,hid).to(device))\n",
    "        \n",
    "        return stat \n",
    "    \n",
    "model ,device ,criterion ,optimizer =define_model( para['embed'] , para['hidden'] ,para['layers'] ,vocab_size , 'adam' ,para['lr'])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xr2Cb8nT1cRt",
    "outputId": "b4c859e7-71a4-4a9f-b04c-408d1a3a0e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step[0/2058], Loss: 10.6228, Perplexity: 41061.87\n",
      "Epoch [1/30], Step[500/2058], Loss: 4.5481, Perplexity: 94.45\n",
      "Epoch [1/30], Step[1000/2058], Loss: 4.3463, Perplexity: 77.19\n",
      "Epoch [1/30], Step[1500/2058], Loss: 4.0456, Perplexity: 57.15\n",
      "Epoch [1/30], Step[2000/2058], Loss: 4.0858, Perplexity: 59.49\n",
      "Epoch [2/30], Step[0/2058], Loss: 4.2854, Perplexity: 72.63\n",
      "Epoch [2/30], Step[500/2058], Loss: 3.6717, Perplexity: 39.32\n",
      "Epoch [2/30], Step[1000/2058], Loss: 3.6281, Perplexity: 37.64\n",
      "Epoch [2/30], Step[1500/2058], Loss: 3.4806, Perplexity: 32.48\n",
      "Epoch [2/30], Step[2000/2058], Loss: 3.3482, Perplexity: 28.45\n",
      "Epoch [3/30], Step[0/2058], Loss: 3.6578, Perplexity: 38.78\n",
      "Epoch [3/30], Step[500/2058], Loss: 2.8998, Perplexity: 18.17\n",
      "Epoch [3/30], Step[1000/2058], Loss: 2.8596, Perplexity: 17.45\n",
      "Epoch [3/30], Step[1500/2058], Loss: 2.8442, Perplexity: 17.19\n",
      "Epoch [3/30], Step[2000/2058], Loss: 2.5870, Perplexity: 13.29\n",
      "Epoch [4/30], Step[0/2058], Loss: 2.8493, Perplexity: 17.28\n",
      "Epoch [4/30], Step[500/2058], Loss: 2.3266, Perplexity: 10.24\n",
      "Epoch [4/30], Step[1000/2058], Loss: 2.4249, Perplexity: 11.30\n",
      "Epoch [4/30], Step[1500/2058], Loss: 2.3388, Perplexity: 10.37\n",
      "Epoch [4/30], Step[2000/2058], Loss: 2.0979, Perplexity:  8.15\n",
      "Epoch [5/30], Step[0/2058], Loss: 2.2804, Perplexity:  9.78\n",
      "Epoch [5/30], Step[500/2058], Loss: 1.9388, Perplexity:  6.95\n",
      "Epoch [5/30], Step[1000/2058], Loss: 2.1389, Perplexity:  8.49\n",
      "Epoch [5/30], Step[1500/2058], Loss: 2.0968, Perplexity:  8.14\n",
      "Epoch [5/30], Step[2000/2058], Loss: 1.8241, Perplexity:  6.20\n",
      "Epoch [6/30], Step[0/2058], Loss: 1.8631, Perplexity:  6.44\n",
      "Epoch [6/30], Step[500/2058], Loss: 1.7183, Perplexity:  5.58\n",
      "Epoch [6/30], Step[1000/2058], Loss: 1.9130, Perplexity:  6.77\n",
      "Epoch [6/30], Step[1500/2058], Loss: 1.8774, Perplexity:  6.54\n",
      "Epoch [6/30], Step[2000/2058], Loss: 1.6726, Perplexity:  5.33\n",
      "Epoch [7/30], Step[0/2058], Loss: 1.6425, Perplexity:  5.17\n",
      "Epoch [7/30], Step[500/2058], Loss: 1.5620, Perplexity:  4.77\n",
      "Epoch [7/30], Step[1000/2058], Loss: 1.7577, Perplexity:  5.80\n",
      "Epoch [7/30], Step[1500/2058], Loss: 1.7548, Perplexity:  5.78\n",
      "Epoch [7/30], Step[2000/2058], Loss: 1.5259, Perplexity:  4.60\n",
      "Epoch [8/30], Step[0/2058], Loss: 1.4515, Perplexity:  4.27\n",
      "Epoch [8/30], Step[500/2058], Loss: 1.4348, Perplexity:  4.20\n",
      "Epoch [8/30], Step[1000/2058], Loss: 1.6510, Perplexity:  5.21\n",
      "Epoch [8/30], Step[1500/2058], Loss: 1.6331, Perplexity:  5.12\n",
      "Epoch [8/30], Step[2000/2058], Loss: 1.5072, Perplexity:  4.51\n",
      "Epoch [9/30], Step[0/2058], Loss: 1.2554, Perplexity:  3.51\n",
      "Epoch [9/30], Step[500/2058], Loss: 1.3307, Perplexity:  3.78\n",
      "Epoch [9/30], Step[1000/2058], Loss: 1.5728, Perplexity:  4.82\n",
      "Epoch [9/30], Step[1500/2058], Loss: 1.5352, Perplexity:  4.64\n",
      "Epoch [9/30], Step[2000/2058], Loss: 1.2989, Perplexity:  3.67\n",
      "Epoch [10/30], Step[0/2058], Loss: 1.1516, Perplexity:  3.16\n",
      "Epoch [10/30], Step[500/2058], Loss: 1.3069, Perplexity:  3.69\n",
      "Epoch [10/30], Step[1000/2058], Loss: 1.5234, Perplexity:  4.59\n",
      "Epoch [10/30], Step[1500/2058], Loss: 1.4558, Perplexity:  4.29\n",
      "Epoch [10/30], Step[2000/2058], Loss: 1.3270, Perplexity:  3.77\n",
      "Epoch [11/30], Step[0/2058], Loss: 1.0882, Perplexity:  2.97\n",
      "Epoch [11/30], Step[500/2058], Loss: 1.1872, Perplexity:  3.28\n",
      "Epoch [11/30], Step[1000/2058], Loss: 1.4555, Perplexity:  4.29\n",
      "Epoch [11/30], Step[1500/2058], Loss: 1.3634, Perplexity:  3.91\n",
      "Epoch [11/30], Step[2000/2058], Loss: 1.2519, Perplexity:  3.50\n",
      "Epoch [12/30], Step[0/2058], Loss: 1.0191, Perplexity:  2.77\n",
      "Epoch [12/30], Step[500/2058], Loss: 1.1726, Perplexity:  3.23\n",
      "Epoch [12/30], Step[1000/2058], Loss: 1.4381, Perplexity:  4.21\n",
      "Epoch [12/30], Step[1500/2058], Loss: 1.3546, Perplexity:  3.88\n",
      "Epoch [12/30], Step[2000/2058], Loss: 1.3385, Perplexity:  3.81\n",
      "Epoch [13/30], Step[0/2058], Loss: 0.9406, Perplexity:  2.56\n",
      "Epoch [13/30], Step[500/2058], Loss: 1.1780, Perplexity:  3.25\n",
      "Epoch [13/30], Step[1000/2058], Loss: 1.4198, Perplexity:  4.14\n",
      "Epoch [13/30], Step[1500/2058], Loss: 1.2847, Perplexity:  3.61\n",
      "Epoch [13/30], Step[2000/2058], Loss: 1.2784, Perplexity:  3.59\n",
      "Epoch [14/30], Step[0/2058], Loss: 0.9156, Perplexity:  2.50\n",
      "Epoch [14/30], Step[500/2058], Loss: 1.1823, Perplexity:  3.26\n",
      "Epoch [14/30], Step[1000/2058], Loss: 1.4311, Perplexity:  4.18\n",
      "Epoch [14/30], Step[1500/2058], Loss: 1.2665, Perplexity:  3.55\n",
      "Epoch [14/30], Step[2000/2058], Loss: 1.2931, Perplexity:  3.64\n",
      "Epoch [15/30], Step[0/2058], Loss: 0.8949, Perplexity:  2.45\n",
      "Epoch [15/30], Step[500/2058], Loss: 1.1360, Perplexity:  3.11\n",
      "Epoch [15/30], Step[1000/2058], Loss: 1.3555, Perplexity:  3.88\n",
      "Epoch [15/30], Step[1500/2058], Loss: 1.1600, Perplexity:  3.19\n",
      "Epoch [15/30], Step[2000/2058], Loss: 1.2367, Perplexity:  3.44\n",
      "Epoch [16/30], Step[0/2058], Loss: 0.8279, Perplexity:  2.29\n",
      "Epoch [16/30], Step[500/2058], Loss: 1.0653, Perplexity:  2.90\n",
      "Epoch [16/30], Step[1000/2058], Loss: 1.3624, Perplexity:  3.91\n",
      "Epoch [16/30], Step[1500/2058], Loss: 1.0991, Perplexity:  3.00\n",
      "Epoch [16/30], Step[2000/2058], Loss: 1.1128, Perplexity:  3.04\n",
      "Epoch [17/30], Step[0/2058], Loss: 0.8764, Perplexity:  2.40\n",
      "Epoch [17/30], Step[500/2058], Loss: 1.0539, Perplexity:  2.87\n",
      "Epoch [17/30], Step[1000/2058], Loss: 1.3120, Perplexity:  3.71\n",
      "Epoch [17/30], Step[1500/2058], Loss: 1.1531, Perplexity:  3.17\n",
      "Epoch [17/30], Step[2000/2058], Loss: 1.1479, Perplexity:  3.15\n",
      "Epoch [18/30], Step[0/2058], Loss: 0.8561, Perplexity:  2.35\n",
      "Epoch [18/30], Step[500/2058], Loss: 1.0999, Perplexity:  3.00\n",
      "Epoch [18/30], Step[1000/2058], Loss: 1.2287, Perplexity:  3.42\n",
      "Epoch [18/30], Step[1500/2058], Loss: 1.1540, Perplexity:  3.17\n",
      "Epoch [18/30], Step[2000/2058], Loss: 1.0883, Perplexity:  2.97\n",
      "Epoch [19/30], Step[0/2058], Loss: 0.8059, Perplexity:  2.24\n",
      "Epoch [19/30], Step[500/2058], Loss: 1.0385, Perplexity:  2.83\n",
      "Epoch [19/30], Step[1000/2058], Loss: 1.1981, Perplexity:  3.31\n",
      "Epoch [19/30], Step[1500/2058], Loss: 1.2462, Perplexity:  3.48\n",
      "Epoch [19/30], Step[2000/2058], Loss: 1.0669, Perplexity:  2.91\n",
      "Epoch [20/30], Step[0/2058], Loss: 0.7657, Perplexity:  2.15\n",
      "Epoch [20/30], Step[500/2058], Loss: 1.0557, Perplexity:  2.87\n",
      "Epoch [20/30], Step[1000/2058], Loss: 1.2840, Perplexity:  3.61\n",
      "Epoch [20/30], Step[1500/2058], Loss: 1.0823, Perplexity:  2.95\n",
      "Epoch [20/30], Step[2000/2058], Loss: 1.0470, Perplexity:  2.85\n",
      "Epoch [21/30], Step[0/2058], Loss: 0.7310, Perplexity:  2.08\n",
      "Epoch [21/30], Step[500/2058], Loss: 1.0093, Perplexity:  2.74\n",
      "Epoch [21/30], Step[1000/2058], Loss: 1.1612, Perplexity:  3.19\n",
      "Epoch [21/30], Step[1500/2058], Loss: 1.0903, Perplexity:  2.98\n",
      "Epoch [21/30], Step[2000/2058], Loss: 0.9861, Perplexity:  2.68\n",
      "Epoch [22/30], Step[0/2058], Loss: 0.7045, Perplexity:  2.02\n",
      "Epoch [22/30], Step[500/2058], Loss: 1.0688, Perplexity:  2.91\n",
      "Epoch [22/30], Step[1000/2058], Loss: 1.1488, Perplexity:  3.15\n",
      "Epoch [22/30], Step[1500/2058], Loss: 1.0994, Perplexity:  3.00\n",
      "Epoch [22/30], Step[2000/2058], Loss: 1.0178, Perplexity:  2.77\n",
      "Epoch [23/30], Step[0/2058], Loss: 0.7076, Perplexity:  2.03\n",
      "Epoch [23/30], Step[500/2058], Loss: 1.0594, Perplexity:  2.88\n",
      "Epoch [23/30], Step[1000/2058], Loss: 1.1193, Perplexity:  3.06\n",
      "Epoch [23/30], Step[1500/2058], Loss: 1.1257, Perplexity:  3.08\n",
      "Epoch [23/30], Step[2000/2058], Loss: 0.9747, Perplexity:  2.65\n",
      "Epoch [24/30], Step[0/2058], Loss: 0.6883, Perplexity:  1.99\n",
      "Epoch [24/30], Step[500/2058], Loss: 1.0570, Perplexity:  2.88\n",
      "Epoch [24/30], Step[1000/2058], Loss: 1.1756, Perplexity:  3.24\n",
      "Epoch [24/30], Step[1500/2058], Loss: 1.1285, Perplexity:  3.09\n",
      "Epoch [24/30], Step[2000/2058], Loss: 0.9824, Perplexity:  2.67\n",
      "Epoch [25/30], Step[0/2058], Loss: 0.7209, Perplexity:  2.06\n",
      "Epoch [25/30], Step[500/2058], Loss: 1.0365, Perplexity:  2.82\n",
      "Epoch [25/30], Step[1000/2058], Loss: 1.0875, Perplexity:  2.97\n",
      "Epoch [25/30], Step[1500/2058], Loss: 1.1166, Perplexity:  3.05\n",
      "Epoch [25/30], Step[2000/2058], Loss: 0.9869, Perplexity:  2.68\n",
      "Epoch [26/30], Step[0/2058], Loss: 0.6833, Perplexity:  1.98\n",
      "Epoch [26/30], Step[500/2058], Loss: 0.9965, Perplexity:  2.71\n",
      "Epoch [26/30], Step[1000/2058], Loss: 1.1178, Perplexity:  3.06\n",
      "Epoch [26/30], Step[1500/2058], Loss: 1.0981, Perplexity:  3.00\n",
      "Epoch [26/30], Step[2000/2058], Loss: 0.9558, Perplexity:  2.60\n",
      "Epoch [27/30], Step[0/2058], Loss: 0.6958, Perplexity:  2.01\n",
      "Epoch [27/30], Step[500/2058], Loss: 0.9661, Perplexity:  2.63\n",
      "Epoch [27/30], Step[1000/2058], Loss: 1.1072, Perplexity:  3.03\n",
      "Epoch [27/30], Step[1500/2058], Loss: 1.0659, Perplexity:  2.90\n",
      "Epoch [27/30], Step[2000/2058], Loss: 0.9277, Perplexity:  2.53\n",
      "Epoch [28/30], Step[0/2058], Loss: 0.6377, Perplexity:  1.89\n",
      "Epoch [28/30], Step[500/2058], Loss: 0.9537, Perplexity:  2.60\n",
      "Epoch [28/30], Step[1000/2058], Loss: 1.0102, Perplexity:  2.75\n",
      "Epoch [28/30], Step[1500/2058], Loss: 1.1265, Perplexity:  3.08\n",
      "Epoch [28/30], Step[2000/2058], Loss: 1.0217, Perplexity:  2.78\n",
      "Epoch [29/30], Step[0/2058], Loss: 0.6345, Perplexity:  1.89\n",
      "Epoch [29/30], Step[500/2058], Loss: 0.9405, Perplexity:  2.56\n",
      "Epoch [29/30], Step[1000/2058], Loss: 1.0880, Perplexity:  2.97\n",
      "Epoch [29/30], Step[1500/2058], Loss: 1.1300, Perplexity:  3.10\n",
      "Epoch [29/30], Step[2000/2058], Loss: 1.0316, Perplexity:  2.81\n",
      "Epoch [30/30], Step[0/2058], Loss: 0.6249, Perplexity:  1.87\n",
      "Epoch [30/30], Step[500/2058], Loss: 0.9182, Perplexity:  2.50\n",
      "Epoch [30/30], Step[1000/2058], Loss: 1.1765, Perplexity:  3.24\n",
      "Epoch [30/30], Step[1500/2058], Loss: 1.0896, Perplexity:  2.97\n",
      "Epoch [30/30], Step[2000/2058], Loss: 0.8439, Perplexity:  2.33\n"
     ]
    }
   ],
   "source": [
    "def detach_hidden(h_stat):\n",
    "    stat_list = []\n",
    "    \n",
    "    for i in h_stat:\n",
    "        stat_list.append(i.detach())\n",
    "    \n",
    "    return stat_list\n",
    "\n",
    "\n",
    "num_epochs = model_para['epoch']\n",
    "for epoch in range(model_para['epoch']):\n",
    "\n",
    "    h_stat = model.get_init_stat(para['layers'] , para['batch'] ,para['hidden'])\n",
    "    \n",
    "    \n",
    "    for i in range(0, train_int.size(1) - para['seq'], spara['seq']):\n",
    "    \n",
    "        ip = train_int[:, i: i+para['seq']]\n",
    "        ip = ip.to(device)\n",
    "        \n",
    "        op = train_int[:, (i+1):(i+1)+para['seq']]\n",
    "        op = op.to(device)\n",
    "        \n",
    "        h_stat = detach_hidden(h_stat)\n",
    "        \n",
    "        out, h_stat = model(ip, h_stat)\n",
    "        model_loss = criterion(out, op.reshape(-1))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        model_loss.backward()\n",
    "        \n",
    "        clip_grad_norm_(model.parameters(), 0.4)\n",
    "        optimizer.step()\n",
    "\n",
    "        fwd_pass = (i+1) // para['seq']\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {}, Perplexity: {}'.format(epoch+1, num_epochs, fwd_pass, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KY1ipv251ft1"
   },
   "outputs": [],
   "source": [
    "def make_test_data( test_data ):\n",
    "    final_data = []\n",
    "\n",
    "    for line in test_data:\n",
    "        temp = ['<s>'] + line.split() + ['</s>']\n",
    "\n",
    "        if len(temp) >= 4:\n",
    "            final_data.append(temp)\n",
    "        \n",
    "    return final_data\n",
    "\n",
    "test_refine = make_test_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "st_z4dZQFJOK"
   },
   "outputs": [],
   "source": [
    "def convert_int(data , word_2_int ):\n",
    "\n",
    "    final = []\n",
    "\n",
    "    for line in data:\n",
    "        temp = []\n",
    "\n",
    "        for w in line:\n",
    "\n",
    "            if w in word_2_int.keys():\n",
    "                temp.append(word_2_int[w])\n",
    "            else:\n",
    "                temp.append(word_2_int['<UNK>'])\n",
    "    \n",
    "        final.append(temp)\n",
    "\n",
    "    return final\n",
    "\n",
    "test_int = convert_int(test_refine , word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "y-EzpomVFXTm"
   },
   "outputs": [],
   "source": [
    "def test_seq( data ,seq_len):\n",
    "\n",
    "    final_input = []\n",
    "    final_output = []\n",
    "\n",
    "    for line in data:\n",
    "\n",
    "        seq_list_ip = []\n",
    "        seq_list_op = []\n",
    "\n",
    "        for i in range(seq_len, len(line) ,seq_len):\n",
    "            t = line[i-seq_len: i]\n",
    "            \n",
    "            seq_list_ip.append( t[:-1] )\n",
    "            seq_list_op.append( t[1:]) \n",
    "        \n",
    "        final_input.append(seq_list_ip)\n",
    "        final_output.append(seq_list_op)\n",
    "\n",
    "    return final_input , final_output \n",
    "\n",
    "test_input , test_output = test_seq( test_int , para['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0CX9bsZQFZQ8"
   },
   "outputs": [],
   "source": [
    "def predict(net , ip ,op , h):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    x = np.array([ip])\n",
    "    inputs = torch.from_numpy(x)\n",
    "    inputs = inputs.cuda()\n",
    "\n",
    "    y = np.array([op])\n",
    "    output = torch.from_numpy(y)\n",
    "    output = output.cuda()\n",
    "\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    loss = criterion(out, output.view(-1))\n",
    "\n",
    "    print( np.exp(loss.item()))\n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdOFSppiFhjZ"
   },
   "outputs": [],
   "source": [
    "def get_perplexity( net , data_ip ,data_op ):\n",
    "\n",
    "    net.eval()\n",
    "    net.cuda()\n",
    "    \n",
    "    h = (torch.zeros(para['layers'], 1, para['hidden']).cuda(),\n",
    "              torch.zeros(para['layers'], 1, para['hidden']).cuda() )\n",
    "\n",
    "    total_loss = 0\n",
    "    for i in range( len(data_ip) ):\n",
    "        total_loss += predict(net, data_ip[i] , data_op[i] , h)\n",
    "\n",
    "    return np.exp(total_loss)\n",
    "\n",
    "preps = []\n",
    "\n",
    "for stmt in range( len(test_input) ):\n",
    "    prp = get_perplexity( model , test_input[stmt] , test_output[stmt] )\n",
    "    preps.append(prp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GyNpgoTEkDhZ",
    "outputId": "34f634b2-badb-45a7-fd1c-8301e00ad8ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg = sum(preps) / len(preps)\n",
    "\n",
    "f = open('lstm.txt', 'a')\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    f.write( test_data[i] +'\\t' + str(preps[i]) + '\\n')\n",
    "\n",
    "f.write(str(avg))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
